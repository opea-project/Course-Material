{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://opea.dev/wp-content/uploads/sites/9/2024/04/opea-horizontal-color.svg\" alt=\"OPEA Logo\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<div style=\"text-align: center; margin: 20px 0;\">\n",
    "<a href=\"https://colab.research.google.com/github/opea-project/Course-Material/blob/main/Curriculas/Kubernetes/AgentQnA/1_Deploy_AgentQnA.ipynb\" target=\"_blank\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/> \n",
    "</a>\n",
    "</div>\n",
    "\n",
    "> **üí° Quick Start:** Click the button above to open this notebook directly in Google Colab - no local setup required!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: #2E86AB; font-weight: bold;\">ü§ñ Building Effective AI Agents</span>\n",
    "\n",
    "## <span style=\"color: #A23B72; font-size: 1.2em;\">‚ö° Deploy OPEA AgentQnA Blueprint on </span><span style=\"color: #F18F01; font-size: 1.2em;\">‚ò∏Ô∏è Kubernetes</span>\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color: #C73E1D;\">üéØ What You'll Learn</span>\n",
    "\n",
    "<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 15px; border-radius: 10px; color: white; margin: 10px 0;\">\n",
    "<strong>üöÄ By the end of this tutorial, you'll be able to deploy production-ready AI agents that can:</strong>\n",
    "<ul>\n",
    "<li>üß† <strong>Think</strong> - Use advanced reasoning capabilities with Large Language Models (LLMs)</li>\n",
    "<li>üîç <strong>Search</strong> - Query knowledge bases intelligently using Retrieval-Augmented Generation (RAG)</li>\n",
    "<li>‚öôÔ∏è <strong>Act</strong> - Execute real-world tasks using external tools and APIs</li>\n",
    "<li>üê≥ <strong>Scale</strong> - Run reliably and efficiently on Kubernetes infrastructure</li>\n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running this notebook, ensure you have the following components set up:\n",
    "\n",
    "## 1. **Jupyter Environment**\n",
    "This notebook can be executed in any of these environments:\n",
    "   - Local Jupyter Notebook/JupyterLab installation\n",
    "   - VS Code with Jupyter extension\n",
    "   - Google Colab (simply upload this notebook file)\n",
    "\n",
    "## 2. **Required Command-Line Tools**\n",
    "Make sure these tools are installed and available in your PATH:\n",
    "   - **`kubectl`** - Kubernetes command-line tool configured with access to your cluster\n",
    "   - **`helm`** - Helm package manager (version 3.x or later) for deploying OPEA charts\n",
    "   - **`docker`** - Container runtime (required if using KIND for local testing)\n",
    "   - **`curl`** and **`jq`** - For testing and parsing API endpoint responses\n",
    "\n",
    "## 3. **Kubernetes Cluster Access**\n",
    "   - A running Kubernetes cluster (can be local KIND cluster, managed cloud cluster, etc.)\n",
    "   - Properly configured `kubeconfig` file to access your cluster\n",
    "   - Sufficient cluster resources (minimum 4GB RAM, 2 CPU cores recommended)\n",
    "   - Cluster should have internet access for pulling container images\n",
    "\n",
    "## 4. **API Keys** (choose one option based on your LLM preference)\n",
    "   - **OpenAI API key** - If you plan to use OpenAI models (GPT-4, GPT-3.5-turbo, etc.)\n",
    "   - **HuggingFace API token** - For accessing open-source models via HuggingFace Hub\n",
    "   - **Remote inference endpoint credentials** - If using custom inference endpoints\n",
    "\n",
    "## 5. **Network Requirements**\n",
    "   - Outbound internet access for downloading Helm charts and container images\n",
    "   - Port-forwarding capabilities for accessing deployed services locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction: Why Build an Agent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 üß† From Talking to Doing: Why We Need More Than Just LLMs\n",
    "\n",
    "Imagine you ask a **consultant**:\n",
    "\n",
    "> ‚ÄúCan you help me plan my trip to Paris?‚Äù\n",
    "\n",
    "They reply:\n",
    "\n",
    "> ‚ÄúSure! You should book a flight, find a hotel near the Eiffel Tower, and maybe buy museum tickets in advance.‚Äù\n",
    "\n",
    "Helpful ‚Äî but **you‚Äôre still doing all the work**.\n",
    "\n",
    "That‚Äôs what a **plain LLM** does: it provides good advice, but doesn‚Äôt take action.\n",
    "\n",
    "\n",
    "Now imagine asking a **personal assistant** the same question:\n",
    "\n",
    "> ‚ÄúCan you plan my trip to Paris?‚Äù\n",
    "\n",
    "They respond:\n",
    "\n",
    "> ‚úÖ Flight booked  \n",
    "> ‚úÖ Hotel reserved  \n",
    "> ‚úÖ Museum tickets purchased  \n",
    "> ‚úÖ Itinerary sent to your inbox\n",
    "\n",
    "That‚Äôs what an **agent** does: it combines language understanding with **tools, APIs, and logic** to get real-world tasks done.\n",
    "\n",
    " \n",
    "- üßë‚Äçüè´ **LLM = Smart talker (consultant)**  \n",
    "- üßë‚Äçüíº **Agent = Smart doer (assistant with tools)**\n",
    "\n",
    "\n",
    "#### **We Need It All ‚Äî LLMs, RAG, and Agents Working Together**\n",
    "\n",
    "| Layer       | What It Brings                              | But It‚Äôs Not Enough‚Ä¶                   |\n",
    "|-------------|----------------------------------------------|----------------------------------------|\n",
    "| **LLMs**    | Powerful at understanding and generating     | Can‚Äôt act or fetch real-time data      |\n",
    "| **RAG**     | Boosts relevance by pulling in context       | Still just text ‚Äî no action            |\n",
    "| **Agents**  | Execute tasks using tools, APIs, and logic   | Need structure, memory, and oversight  |\n",
    "\n",
    "To build real-world AI systems, we need **everything**, because:\n",
    "\n",
    "- üîó **External sources matter** ‚Äî APIs, databases, real-time feeds  \n",
    "- ‚öôÔ∏è **Action matters** ‚Äî booking, triggering, integrating  \n",
    "- üßë‚Äçüíº **Expertise matters** ‚Äî agents must specialize by domain\n",
    "\n",
    "> **Agents aren‚Äôt just smarter ‚Äî they *do*.**  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 üß∞ Tools, Workers, and Agents ‚Äî The Execution Layer\n",
    "\n",
    "> üí° **Agents aren‚Äôt just text generators ‚Äî they _use tools_, delegate tasks to _workers_, and execute complex workflows.**\n",
    "\n",
    "- **Tools**: Interfaces for querying APIs, triggering functions, or transforming data (e.g., search, calculator, database access).\n",
    "- **Workers**: Specialized sub-agents or microservices that **retrieve**, **analyze**, or **transform** data for specific tasks.\n",
    "- **Agents**: Decision-makers that orchestrate tools and workers to achieve a goal. They follow predefined rules or learned policies.\n",
    "\n",
    "Agents can **interact with users**, **retrieve and process data**, and **execute tasks** based on rules or learning models.\n",
    "\n",
    "### üîë Key Features of AI Agents:\n",
    "- ‚úÖ **Autonomous** ‚Äì Operate with minimal human intervention  \n",
    "- ‚úÖ **Perceptual** ‚Äì Analyze information from text, voice, images, or structured data  \n",
    "- ‚úÖ **Goal-oriented** ‚Äì Work to accomplish tasks such as answering questions or summarizing content  \n",
    "- ‚úÖ **Interactive** ‚Äì Communicate with humans or with other agents  \n",
    "- ‚úÖ **Adaptive** ‚Äì Improve over time through learning or feedback  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Types of Agents**  \n",
    "Agents can be classified based on their reasoning and decision-making capabilities:\n",
    "\n",
    "| Type | Description | Example |\n",
    "|------|-------------|---------|\n",
    "| **Reactive Agents** | Respond only to current inputs, without memory | Chatbot answering direct questions |\n",
    "| **Goal-Based Agents** | Use goals to guide their decisions | Recommendation systems |\n",
    "| **Utility-Based Agents** | Optimize for the best possible decision | Self-driving cars adjusting speed |\n",
    "| **Learning Agents** | Improve with data and experience | AI models fine-tuned for specific tasks |\n",
    "\n",
    "#### **Example: Difference Between Reactive and Goal-Based Agents**  \n",
    "üöó **Reactive Agent:** A self-driving car stops when the traffic light is red but doesn‚Äôt plan ahead.  \n",
    "üõ£Ô∏è **Goal-Based Agent:** The car not only stops but also adjusts its speed to optimize fuel efficiency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Example of an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Clone Course Material\n",
    "!pip install -r https://raw.githubusercontent.com/opea-project/Course-Material/main/Curriculas/Kubernetes/AgentQnA/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **üí° NOTE:** Add your OpenAI key to test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.tools import Tool\n",
    "import os\n",
    "\n",
    "# Create a function for mathematical calculations\n",
    "def calculate(input_str):\n",
    "    \"\"\"Evaluates simple mathematical expressions\"\"\"\n",
    "    try:\n",
    "        return str(eval(input_str))\n",
    "    except Exception as e:\n",
    "        return f\"Calculation error: {e}\"\n",
    "\n",
    "# Initialize the language model\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    openai_api_key=\"your openai_api_key\"\n",
    ")\n",
    "\n",
    "# Use the Tool method\n",
    "Math_tool = Tool(\n",
    "    name=\"Calculator\",\n",
    "    func=calculate,\n",
    "    description=\"Useful ONLY for calculating simple math expressions. Expected input: '2+2'\"\n",
    ")\n",
    "\n",
    "# You can try removing the word ONLY and see the difference.\n",
    "# The model only has one tool and will try to use it.\n",
    "# When an agent has limited tools, it may try to use whatever it has‚Äîeven if it's not appropriate.\n",
    "\n",
    "# Initialize the agent with additional tools\n",
    "agent = initialize_agent(\n",
    "    tools=[Math_tool],\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "# Test the agent\n",
    "print(agent.run(\"What is the capital of France?\"))   # This is a non-math question\n",
    "print(agent.run(\"What is 12 * 8?\"))                  # This is a math question\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 üß† From Language to Action: The Challenge of Real AI Agents\n",
    "\n",
    "Large Language Models (LLMs) are excellent at generating natural language ‚Äî but real-world applications demand more than just fluent responses. To truly assist users, AI systems must:\n",
    "\n",
    "- Use tools and APIs to take action\n",
    "- Retain and reason across multiple steps\n",
    "- Query databases or retrieve relevant documents\n",
    "- Operate reliably in production environments\n",
    "\n",
    "> Building these capabilities from scratch is complex. Most frameworks either oversimplify agent logic or lock you into rigid architectures that are hard to scale or customize.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. OPEA and Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**OPEA (Open Platform for Enterprise Agents)** is an open source framework designed to simplify the development and deployment of production-ready GenAI deployments.\n",
    "\n",
    "It gives you the scaffolding to go beyond chat and build systems that can reason, act, and specialize.\n",
    "\n",
    "\n",
    "## 2.1 Why Use OPEA?\n",
    "\n",
    "| Feature                      | What It Enables                                |\n",
    "|-----------------------------|-------------------------------------------------|\n",
    "| **Agentic Blueprints**       | Skip boilerplate code ‚Äî use prebuilt, customizable agent workflows (RAG, tool use, multi-agent coordination) |\n",
    "| **Modular by Design**        | Easily swap in any LLM, memory backend, or external tool without major code changes |\n",
    "| **Deployment-Ready**         | Includes Docker containers, REST APIs, distributed tracing, and monitoring out-of-the-box |\n",
    "| **Enterprise-Friendly**      | Secure, observable, and compatible with on-premises or cloud deployments |\n",
    "| **Community-Driven**         | Actively maintained by contributors from Intel and the broader open-source ecosystem |\n",
    "\n",
    "\n",
    "### üîç Built for Real Applications\n",
    "\n",
    "OPEA bridges the gap between research prototypes and production-ready deployments:\n",
    "\n",
    "- **Vector Database Integration**: Connects seamlessly to vector databases like Chroma, Qdrant, and Weaviate for efficient similarity search\n",
    "- **Stateful Workflows**: Supports persistent memory and multi-step agent workflows with conversation history\n",
    "- **Flexible Tool Integration**: Enables tool calling via REST APIs, SQL databases, or custom function schemas\n",
    "- **Production Observability**: Provides comprehensive logging, tracing, and debugging capabilities for agent workflows\n",
    "- **Scalable Architecture**: Designed to handle enterprise workloads with Kubernetes-native scaling and load balancing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3  Deploy OPEA AgentQnA in Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook assumes you have a Kubernetes cluster already deployed and configured. You can follow this guide to deploy it on any platform.\n",
    "\n",
    "**What is AgentQnA?** \n",
    "AgentQnA is an OPEA blueprint that demonstrates a multi-agent Question & Answer system. It includes:\n",
    "- A **Supervisor Agent** that routes questions to specialized worker agents\n",
    "- A **RAG Agent** that retrieves information from knowledge bases  \n",
    "- A **SQL Agent** that queries databases using natural language\n",
    "- Tool integration for web search, calculations, and more\n",
    "\n",
    "This example showcases a hierarchical multi-agent system for question-answering applications. The architecture diagram below shows a supervisor agent that interfaces with the user and dispatches tasks to two worker agents to gather information and come up with answers. The worker RAG agent uses the retrieval tool to retrieve relevant documents from a knowledge base - a vector database. The worker SQL agent retrieves relevant data from a SQL database. Although not included in this example by default, other tools such as a web search tool or a knowledge graph query tool can be used by the supervisor agent to gather information from additional sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![agent_qna_arch.png](https://github.com/opea-project/GenAIExamples/raw/main/AgentQnA/assets/img/agent_qna_arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Verify Kubernetes Connection\n",
    "\n",
    "Before proceeding, let's verify your Kubernetes cluster connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download your kubeconfig access \n",
    "\n",
    "!export KUBECONFIG=/path/to/your/kubeconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify kubectl connection and cluster info\n",
    "!kubectl cluster-info\n",
    "!kubectl get nodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Download Required Resources\n",
    "\n",
    "Let's start by cloning the necessary repository and downloading agent configuration files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone GenAIExamples repo, you will need it for later\n",
    "\n",
    "!git clone https://github.com/opea-project/GenAIExamples.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Understanding AgentQnA Architecture\n",
    "\n",
    "AgentQnA follows a **multi-agent architecture** with specialized roles:\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[User Query] --> B[Supervisor Agent]\n",
    "    B --> C{Route Decision}\n",
    "    C -->|Knowledge Question| D[RAG Agent]\n",
    "    C -->|Database Question| E[SQL Agent]\n",
    "    C -->|General Question| F[Direct Response]\n",
    "    D --> G[Vector Database]\n",
    "    E --> H[SQL Database]\n",
    "    D --> I[Response]\n",
    "    E --> I\n",
    "    F --> I\n",
    "    I --> J[User]\n",
    "```\n",
    "\n",
    "**Key Components:**\n",
    "- **Supervisor Agent**: Analyzes incoming questions and routes them to appropriate specialized agents\n",
    "- **RAG Agent**: Handles knowledge-based questions by retrieving relevant documents from vector databases\n",
    "- **SQL Agent**: Converts natural language to SQL queries for structured data retrieval\n",
    "- **Tool Integration**: Agents can use external tools like web search, calculators, and APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Agents configurations locally  \n",
    "import os\n",
    "\n",
    "# Get current working directory\n",
    "WORKDIR = os.getcwd()\n",
    "\n",
    "# Define the target directory\n",
    "tools_dir = os.path.join(WORKDIR, \"mnt\", \"tools\")\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(tools_dir, exist_ok=True)\n",
    "\n",
    "# Download the files to the correct directory\n",
    "!wget https://raw.githubusercontent.com/opea-project/GenAIExamples/refs/heads/main/AgentQnA/tools/supervisor_agent_tools.yaml -O {tools_dir}/supervisor_agent_tools.yaml\n",
    "!wget https://raw.githubusercontent.com/opea-project/GenAIExamples/refs/heads/main/AgentQnA/tools/tools.py -O {tools_dir}/tools.py\n",
    "!wget https://raw.githubusercontent.com/opea-project/GenAIExamples/refs/heads/main/AgentQnA/tools/pycragapi.py -O {tools_dir}/pycragapi.py\n",
    "\n",
    "!wget https://raw.githubusercontent.com/opea-project/GenAIExamples/refs/heads/main/AgentQnA/tools/worker_agent_tools.yaml -O {tools_dir}/worker_agent_tools.yaml\n",
    "!wget https://raw.githubusercontent.com/opea-project/GenAIExamples/refs/heads/main/AgentQnA/tools/worker_agent_tools.py -O {tools_dir}/worker_agent_tools.py\n",
    "\n",
    "!wget https://raw.githubusercontent.com/lerocha/chinook-database/refs/heads/master/ChinookDatabase/DataSources/Chinook_Sqlite.sqlite -O {tools_dir}/Chinook_Sqlite.sqlite\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Prepare Configuration Files\n",
    "### Option 1: For KIND (Kubernetes in Docker) Clusters\n",
    "\n",
    "If you're using a KIND cluster, we need to copy the downloaded tools into the cluster's filesystem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully copied 1.1MB to kind-control-plane:/mnt/tools/\n",
      "‚úÖ All tools copied into kind-control-plane:/mnt/tools/\n"
     ]
    }
   ],
   "source": [
    "# Add Agents tools and configuration to control-plane\n",
    "\n",
    "import os\n",
    "\n",
    "# Define local tools directory\n",
    "WORKDIR = os.getcwd()\n",
    "TOOLS_DIR = os.path.join(WORKDIR, \"mnt/tools\")\n",
    "\n",
    "# Create destination folder in the kind node\n",
    "!docker exec kind-control-plane mkdir -p /mnt/tools\n",
    "\n",
    "if os.system(f\"docker cp {TOOLS_DIR}/. kind-control-plane:/mnt/tools/\") == 0:\n",
    "    print(\"‚úÖ Tools copied successfully\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to copy tools\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: For Regular Kubernetes Clusters\n",
    "\n",
    "For non-KIND clusters, you'll need to create a persistent volume or use an alternative method to make the tools available to your pods. This could involve:\n",
    "- Creating a ConfigMap with the tool files\n",
    "- Using a persistent volume claim\n",
    "- Building the tools into your container images "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Configure Your LLM Engine\n",
    "\n",
    "Next, we'll download the Helm chart and configure it for your specific LLM provider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulled: ghcr.io/opea-project/charts/agentqna:1.3.0\n",
      "Digest: sha256:6ae324080c6af874964d74bf7c9e35bfb599bf00ba5d320ed3a64c6283dd6d5c\n",
      "Error: failed to untar: a file or directory with the name agentqna already exists\n"
     ]
    }
   ],
   "source": [
    "!helm pull oci://ghcr.io/opea-project/charts/agentqna --version 0-latest --untar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate to your variant configuration directory `/agentqna/`. You'll find several pre-configured options:\n",
    "\n",
    "![image](./Images/variant.png)\n",
    "\n",
    "The variant files contain different configurations for various LLM providers and deployment scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have multiple options for where to run your LLM:\n",
    "\n",
    "- **OpenAI**: To use OpenAI models, generate a key following [these instructions](https://platform.openai.com/api-keys)\n",
    "- **Local Models (Open Source)**: Deploy models locally using TGI, vLLM, or similar inference engines\n",
    "- **Remote Inference (OpenAI-Compatible)**: Use third-party inference providers with OpenAI-compatible APIs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Remote Inference (OpenAI-Compatible APIs):**\n",
    "\n",
    "Edit the variant configuration file (`agentqna/variant_openai-values.yaml`) to set all agents to use your remote inference endpoint:\n",
    "\n",
    "```yaml\n",
    "supervisor:\n",
    "  model: \"meta-llama/Llama-3.3-70B-Instruct\"                                                  #<-----LLM model used by the agent\n",
    "  llm_endpoint_url: \"https://api.inference.denvrdata.com\"                                     #<-----OpenAI Like endpoint URL\n",
    "  llm_engine: openai\n",
    "  OPENAI_API_KEY: \"Your_remote_inference_key\"\n",
    "\n",
    "ragagent:\n",
    "  model: \"meta-llama/Llama-3.3-70B-Instruct\" \n",
    "  llm_endpoint_url: \"https://api.inference.denvrdata.com\"\n",
    "  OPENAI_API_KEY: \"Your_remote_inference_key\"\n",
    "  llm_engine: openai\n",
    "\n",
    "sqlagent:\n",
    "  model: \"meta-llama/Llama-3.3-70B-Instruct\" \n",
    "  llm_endpoint_url: \"https://api.inference.denvrdata.com\"\n",
    "  llm_engine: openai\n",
    "  OPENAI_API_KEY: \"Your_remote_inference_key\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For OpenAI Models:**\n",
    "\n",
    "If you're using OpenAI, modify all agents in your variant file like this:\n",
    "\n",
    "```yaml\n",
    "supervisor:\n",
    "  model: \"gpt-4o-mini-2024-07-18\"\n",
    "  llm_engine: openai\n",
    "  OPENAI_API_KEY: \"your_openai_key\"\n",
    "\n",
    "ragagent:\n",
    "  model: \"gpt-4o-mini-2024-07-18\"\n",
    "  llm_engine: openai\n",
    "  OPENAI_API_KEY: \"your_openai_key\"\n",
    "\n",
    "sqlagent:\n",
    "  model: \"gpt-4o-mini-2024-07-18\"\n",
    "  llm_engine: openai\n",
    "  OPENAI_API_KEY: \"your_openai_key\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Deploy the Helm Chart\n",
    "\n",
    "Now we'll deploy AgentQnA to your Kubernetes cluster using Helm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release \"agentqna\" does not exist. Installing it now.\n",
      "NAME: agentqna\n",
      "LAST DEPLOYED: Thu Jun  5 00:59:49 2025\n",
      "NAMESPACE: default\n",
      "STATUS: deployed\n",
      "REVISION: 1\n"
     ]
    }
   ],
   "source": [
    "# Deploy Helm chart using your configured variant\n",
    "# Replace \"your_hf_key\" with your actual HuggingFace API token\n",
    "!helm upgrade --install agentqna agentqna \\\n",
    "  -f agentqna/variant_openai-values.yaml \\\n",
    "  --set global.HUGGINGFACEHUB_API_TOKEN=\"your_hf_key\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                        READY   STATUS    RESTARTS   AGE\n",
      "agentqna-agentqna-ui-757c79dcf9-wq8js       1/1     Running   0          17h\n",
      "agentqna-crag-6b876d6788-jtq25              1/1     Running   0          17h\n",
      "agentqna-data-prep-7d6964b5b7-rfq76         1/1     Running   0          17h\n",
      "agentqna-docretriever-65f8b7dc74-vkxzj      1/1     Running   0          17h\n",
      "agentqna-embedding-usvc-76959d7967-bf964    1/1     Running   0          17h\n",
      "agentqna-ragagent-cbcc5d88-8fpvm            1/1     Running   0          17h\n",
      "agentqna-redis-vector-db-7b5d76c94c-9pcn9   1/1     Running   0          17h\n",
      "agentqna-reranking-usvc-6f9c775475-tl9jm    1/1     Running   0          17h\n",
      "agentqna-retriever-usvc-7bd7f8c595-7q6q4    1/1     Running   0          17h\n",
      "agentqna-sqlagent-6cdcf5b54d-hg2n8          1/1     Running   0          17h\n",
      "agentqna-supervisor-54cc4648bd-ppd2z        1/1     Running   0          17h\n",
      "agentqna-tei-55c54b8cbb-txqlj               1/1     Running   0          17h\n",
      "agentqna-teirerank-7d6b45bff9-wxwqz         1/1     Running   0          17h\n"
     ]
    }
   ],
   "source": [
    "# Check pod status - ALL CONTAINERS MUST BE IN RUNNING STATE (1/1) \n",
    "# This can take approximately 10 minutes for all images to download and pods to start\n",
    "!kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Set Up Port Forwarding\n",
    "\n",
    "Now we'll set up port forwarding to access the Supervisor microservice locally on port 9090:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Check if port 9090 is already open\n",
    "def is_port_open(host='localhost', port=9090):\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "        sock.settimeout(1)  # short timeout for responsiveness in notebook\n",
    "        try:\n",
    "            sock.connect((host, port))\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "if is_port_open():\n",
    "    print(\"‚úÖ Port-forward already running on localhost:9090.\")\n",
    "else:\n",
    "    print(\"üîÅ Port not open. Starting port-forward to svc/agentqna-supervisor...\")\n",
    "    subprocess.Popen(\n",
    "        [\"kubectl\", \"port-forward\", \"svc/agentqna-supervisor\", \"9090:9090\"],\n",
    "        stdout=subprocess.DEVNULL,\n",
    "        stderr=subprocess.DEVNULL\n",
    "    )\n",
    "    # Optional: wait a moment to let it start\n",
    "    time.sleep(2)\n",
    "    if is_port_open():\n",
    "        print(\"‚úÖ Port-forward started successfully.\")\n",
    "    else:\n",
    "        print(\"‚ùå Failed to start port-forward.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling connection for 9090\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Check Behavior\n",
    "\n",
    "!curl -s http://localhost:9090/v1/chat/completions \\\n",
    "  -X POST \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"messages\": \"2+2\"}' | jq -r '.text'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Ingest Documents for the RAG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['kubectl', 'port-forward', 'svc/agentqna-dat...>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forwarding from 127.0.0.1:6007 -> 5000\n",
      "Forwarding from [::1]:6007 -> 5000\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.Popen([\"kubectl\", \"port-forward\", \"svc/agentqna-data-prep\", \"6007:6007\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export WORKDIR=$(pwd) && \\\n",
    "export host_ip=localhost && \\\n",
    "cd GenAIExamples/AgentQnA/retrieval_tool/ && \\\n",
    "bash run_ingest_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This information will be used by the RAG agent\n",
    "!cat GenAIExamples/AgentQnA/example_data/test_docs_music.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Validate Agent Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the different agent behaviors with various types of questions: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Test Supervisor with a General Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling connection for 9090\n",
      "Deep learning is a subset of machine learning that uses neural networks with many layers (hence 'deep') to analyze various forms of data. It is particularly effective in tasks such as image and speech recognition, natural language processing, and autonomous driving. Deep learning models learn from large amounts of data, automatically extracting features and patterns without the need for manual feature engineering.\n"
     ]
    }
   ],
   "source": [
    "!curl -s http://localhost:9090/v1/chat/completions \\\n",
    "  -X POST \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"messages\": \"What is Deep Learning?\"}' | jq -r '.text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl logs -l app.kubernetes.io/name=supervisor --tail=30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Test Question That Should Route to RAG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling connection for 9090\n",
      "Rod Temperton\n"
     ]
    }
   ],
   "source": [
    "# Check RAG Agent \n",
    "\n",
    "!curl -s http://localhost:9090/v1/chat/completions \\\n",
    "  -X POST \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"messages\": \"Who wrote the song Thriller?\"}' | jq -r '.text'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Test Question That Should Route to SQL Agent\n",
    "\n",
    "Now let's test a question that should be routed to the SQL agent for database queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CRAG\n",
    "\n",
    "!curl -s http://localhost:9090/v1/chat/completions \\\n",
    "  -X POST \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"messages\": \"Grammy Best New Artist for 2020?\"}' | jq -r '.text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CRAG\n",
    "\n",
    "!curl -s http://localhost:9090/v1/chat/completions \\\n",
    "  -X POST \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"messages\": \"Grammy Best New Artist for 2013?\"}' | jq -r '.text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check SQL DataBase\n",
    "\n",
    "!curl -s http://localhost:9090/v1/chat/completions \\\n",
    "  -X POST \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"messages\": \"How many albums does Iron Maiden have?\"}' | jq -r '.text'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Change Thread ID for Conversation Management\n",
    "\n",
    " The thread ID is an important parameter that helps manage separate conversations with the agent. By specifying a thread ID, you can:\n",
    "\n",
    " - Maintain conversation context across multiple messages\n",
    " - Have multiple independent conversations simultaneously \n",
    " - Reference and continue previous conversations\n",
    " - Track conversation history for specific interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s http://localhost:9090/v1/chat/completions \\\n",
    "  -X POST \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"role\": \"user\", \"messages\": \"How many albums does Iron Maiden have?\", \"thread_id\": \"abc123\", \"stream\": false}' \\\n",
    "  | jq -r '.text'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# üéâ Congratulations!\n",
    "\n",
    "You have successfully deployed and tested OPEA AgentQnA on Kubernetes! You now have:\n",
    "\n",
    "‚úÖ **A multi-agent system** that can route questions intelligently  \n",
    "‚úÖ **RAG capabilities** for knowledge-based queries  \n",
    "‚úÖ **SQL agent** for database interactions  \n",
    "‚úÖ **Scalable deployment** on Kubernetes infrastructure  \n",
    "‚úÖ **Production-ready architecture** with monitoring and observability\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Experiment with different types of questions to see how the supervisor routes them\n",
    "- Add your own documents to the RAG agent's knowledge base\n",
    "- Customize the agent prompts for your specific use cases\n",
    "- Explore the OPEA documentation for more advanced configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Information: How AgentQnA Works Under the Hood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReAct Methodology\n",
    "\n",
    "The Supervisor agent uses the **ReAct (Reasoning and Acting)** methodology, which follows this pattern: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thought ‚Üí Action ‚Üí Observation ‚Üí Thought ‚Üí ...**\n",
    "\n",
    "This iterative process allows the agent to:\n",
    "1. **Think** about what to do next\n",
    "2. **Act** by using available tools or calling sub-agents\n",
    "3. **Observe** the results of the action\n",
    "4. **Think** again about the next step based on new information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPEA Agent Implementation\n",
    "\n",
    "OPEA implements this supervisor logic in the codebase at:\n",
    "`GenAIComps/comps/agent/src/integrations/strategy/react/prompt.py`\n",
    "\n",
    "The implementation follows these steps:\n",
    "1. **Create the prompt** with the ReAct system message and user query\n",
    "2. **Call the LLM** using `llm.invoke()` to get reasoning and action decisions\n",
    "3. **Execute tools** based on the agent's decision\n",
    "4. **Build the agent's toolkit** with available tools and sub-agents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example System Prompt\n",
    "\n",
    "Here's an example of the ReAct system message used by OPEA agents:\n",
    "```\n",
    "REACT_SYS_MESSAGE = \"\"\"\\\n",
    "Decompose the user request into a series of simple tasks when necessary and solve the problem step by step.\n",
    "When you cannot get the answer at first, do not give up. Reflect on the info you have from the tools and try to solve the problem in a different way.\n",
    "Please follow these guidelines when formulating your answer:\n",
    "1. If the question contains a false premise or assumption, answer ‚Äúinvalid question‚Äù.\n",
    "2. If you are uncertain or do not know the answer, respond with ‚ÄúI don‚Äôt know‚Äù.\n",
    "3. Give concise, factual and relevant answers.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "Steps are:\n",
    "- Create the prompt\n",
    "- Call the llm, `llm.invoke`\n",
    "\n",
    "\n",
    "Build the tool set the agent have available, in this case \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "This system prompt guides the agent to:\n",
    "- Break down complex requests into manageable steps\n",
    "- Persist through initial failures by trying different approaches  \n",
    "- Maintain accuracy by acknowledging uncertainty when appropriate\n",
    "- Provide concise, relevant responses\n",
    "\n",
    "## Agent Implementation Workflow\n",
    "\n",
    "The agent implementation follows this workflow:\n",
    "1. **Create the prompt** with the ReAct system message and user query\n",
    "2. **Call the LLM** using `llm.invoke()` to get reasoning and action decisions\n",
    "3. **Execute tools** based on the agent's decision\n",
    "4. **Build the agent's toolkit** with available tools and sub-agents\n",
    "\n",
    "Each agent in AgentQnA has access to specialized tools that enable them to perform their specific functions, whether that's querying vector databases, executing SQL commands, or calling external APIs.\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
